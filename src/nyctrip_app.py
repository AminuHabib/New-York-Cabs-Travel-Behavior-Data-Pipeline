# -*- coding: utf-8 -*-
"""nyctrip-app.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-PM9hkz4NdvWsuhxigyvs_qC349x4Y0R

**Connecting Drive to Colab** \
Mounting Google Drive
"""

from google.colab import drive
drive.mount('/content/drive')

"""**Reading Data from Drive** \
Unzipping the data
"""

!unzip "/content/drive/MyDrive/New-York-Data/train.zip"

"""**Setting up PySpark in Colab** """

!apt-get install openjdk-8-jdk-headless -qq > /dev/null

"""Installing Apache Spark 3.0.1 with Hadoop 2.7 from the link"""

!wget -q https://downloads.apache.org/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz

"""To unzip that folder"""

!tar xf spark-3.1.1-bin-hadoop2.7.tgz

"""Install findspark library """

!pip install -q findspark

"""To set the environment path"""

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.1.1-bin-hadoop2.7"

"""To locate Spark in the system"""

import findspark
findspark.init()

"""To know the location where Spark is installed"""

findspark.find()

"""To view the Spark UI"""

!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip
!unzip ngrok-stable-linux-amd64.zip
get_ipython().system_raw('./ngrok http 4050 &')
!curl -s http://localhost:4040/api/tunnels

from pyspark.sql import SparkSession

spark = SparkSession.builder\
        .master("local[*]")\
        .appName("New York Cab Info")\
        .config('spark.ui.port', '4050')\
        .getOrCreate()
print("A Technical Case Study of New York Cab Trips")

# os.environ['PYSPARK_SUBMIT_ARGS']= spark.sparkContext.addPyFile('../')

spark

"""Loading data into PySpark"""

data = spark.read.csv("train.csv", header=True, inferSchema=True)

"""Understanding the Data"""

data.printSchema()

"""To display the informations"""

data.show(5)

"""To count the number of rows"""

data.count()

data.select("pickup_datetime","dropoff_datetime").show(5)

data.describe().show()

#from pyspark.sql.column import Column as col, _to_java_column, _to_seq

from pyspark.sql.types import *

"""Converting into timestamp"""

data_conv = data.withColumn("pickup_datetime", data["pickup_datetime"].cast(TimestampType()))

data_conv2 = data_conv.withColumn("dropoff_datetime", data_conv["dropoff_datetime"].cast(TimestampType()))

data_conv2.printSchema()

import pyspark.sql.functions as f

"""New columns to show the pickup and dropoff days"""

data_conv3 = data_conv2.withColumn("pickup_day", f.date_format("pickup_datetime", "EEEE"))

data_conv3.show(5)

data_conv4 = data_conv3.withColumn("dropoff_day", f.date_format("dropoff_datetime", "EEEE"))

data_conv4.show()

"""New columns to show the pickup and dropoff day numbers"""

data_conv5 = data_conv4.withColumn("pickup_day_no", f.date_format("pickup_datetime", "F").cast(IntegerType()))

data_conv6 = data_conv5.withColumn("dropoff_day_no", f.date_format("dropoff_datetime", "F").cast(IntegerType()))

data_conv6.printSchema()

"""New columns to show the pickup and dropoff hours"""

data_conv7 = data_conv6.withColumn("pickup_hour", f.date_format("pickup_datetime", "H").cast(IntegerType()))

data_conv8 = data_conv7.withColumn("dropoff_hour", f.date_format("dropoff_datetime", "H").cast(IntegerType()))

data_conv8.printSchema()

"""New columns to show the pickup and dropoff months"""

data_conv9 = data_conv8.withColumn("pickup_month", f.date_format("pickup_datetime", "M").cast(IntegerType()))

data_conv10 = data_conv9.withColumn("dropoff_month", f.date_format("dropoff_datetime", "M").cast(IntegerType()))

data_conv10.printSchema()

from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType, StringType
from pyspark.sql.functions import udf
from pyspark.sql import Row

"""A User Defined function to calculate time of day per 4-hour period"""

def time_of_day(x):
    if x in range(6,10):
        return "Morning"
    elif x in range(10,14):
        return "Afternoon"
    elif x in range(14,18):
        return "Evening"
    else:
        return "Night"

col_time_of_day = udf(lambda z: time_of_day(z))
spark.udf.register("col_time_of_day", time_of_day, StringType())

"""New columns to show the pickup and dropoff time of the day"""

data_conv11 = data_conv10.withColumn("pickup_timeofday", col_time_of_day("pickup_hour"))

data_conv12 = data_conv11.withColumn("dropoff_timeofday", col_time_of_day("dropoff_hour"))

data_conv12.show()

data_conv12.printSchema()

from geopy.distance import great_circle

"""A function to calculate the distance between two coordinates"""

def cal_distance(pickup_lat , pickup_long , dropoff_lat, dropoff_long):
    start_coordinates = pickup_lat, pickup_long
    stop_coordinates = dropoff_lat, dropoff_long

    return great_circle(start_coordinates, stop_coordinates).km

cal_distance_udf = udf(lambda x1,x2,y1,y2: cal_distance(x1,x2,y1,y2))
spark.udf.register("cal_distance_udf", cal_distance, DoubleType())

"""New column to show the distances in km of trips"""

data_conv13 = data_conv12.withColumn("distance", cal_distance_udf("pickup_latitude", "pickup_longitude", "dropoff_latitude", "dropoff_longitude"))

data_conv13.show()

"""Sql query to display the total trips"""

data_conv13.createOrReplaceTempView("data_conv13")
spark.sql("SELECT COUNT(id) AS total_trip from data_conv13").show()

"""To show the number of trips made according to the day of the week using id column"""

spark.sql("SELECT pickup_day, COUNT(id) AS total_trips FROM data_conv13 GROUP BY pickup_day ORDER BY total_trips DESC").show()

"""number of trips made according to the day of the week using vendor_id column"""

spark.sql("SELECT pickup_day, COUNT(vendor_id) AS total_trips FROM data_conv13 GROUP BY pickup_day ORDER BY total_trips DESC").show()

"""To show the number of trips made according to the time of day using vendor_id column"""

spark.sql("SELECT pickup_timeofday, COUNT(vendor_id) AS total_trips_time_of_day FROM data_conv13 GROUP BY pickup_timeofday ORDER BY total_trips_time_of_day DESC").show()

"""To show the number of trips made according to the time of day"""

spark.sql("SELECT pickup_timeofday, COUNT(id) AS total_trips_time_of_day FROM data_conv13 GROUP BY pickup_timeofday ORDER BY total_trips_time_of_day DESC").show()

"""To show the number of km traveled per day of the week"""

spark.sql("SELECT pickup_day, SUM(distance) AS km_traveled_per_day FROM data_conv13 GROUP BY pickup_day ORDER BY km_traveled_per_day DESC").show()

"""Save to file"""

data_conv13.write.csv("/content/drive/MyDrive/New-York-Data/processed_final_data.csv", header=True)

"""To display the number of partitions"""

data_conv13.rdd.getNumPartitions()
